<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Notes on coding]]></title><description><![CDATA[Texts about the programming I'm doing]]></description><link>https://AwesomeLemon.github.io</link><image><url>https://pp.vk.me/c628723/v628723459/2bfc3/FGsR1V_CMvg.jpg</url><title>Notes on coding</title><link>https://AwesomeLemon.github.io</link></image><generator>RSS for Node</generator><lastBuildDate>Sun, 15 Jan 2017 11:10:57 GMT</lastBuildDate><atom:link href="https://AwesomeLemon.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Document recognition with Python, OpenCV and Tesseract]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Recently I’ve conducted my own little experiment with the document recognition technology: I’ve successfully went from an image to the recognized editable text.<br>
On the way I heavily relied on the two following articles:</p>
</div>
<div class="paragraph">
<p>1) <a href="http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/" class="bare">http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/</a><br>
2) <a href="http://www.danvk.org/2015/01/07/finding-blocks-of-text-in-an-image-using-python-opencv-and-numpy.html" class="bare">http://www.danvk.org/2015/01/07/finding-blocks-of-text-in-an-image-using-python-opencv-and-numpy.html</a></p>
</div>
<div class="paragraph">
<p>However, I’ve added something myself, and that’s what I want to write about: the ways to improve upon the given articles to achieve the goal of recognizing plain text from photos at arbitrary angles and illumination.</p>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_skew_removal_first_article">Skew removal (first article)</h5>
<div class="ulist">
<ul>
<li>
<p>If the edges in your image aren’t recognized properly, it makes sense to move interval in Canny function to the left. For example, I’ve settled on [40, 150], instead of [75, 200] proposed in the article.</p>
</li>
<li>
<p>To be able to recognize documents with less straight than rectangular shapes (for example, with curved corners), you should apply blurring to the image with detected edges. I found that it improves approximation quality even in some cases of “good” document shapes, and decreases in none.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Resulting image at this point can be fed to the Tesseract engine, but to get better results out of it, we can conduct two more steps:</p>
</div>
</div>
<div class="sect4">
<h5 id="_noise_removal">Noise removal</h5>
<div class="paragraph">
<p>I’ve experimented with different combinations of Gaussian blur, median blur, erosion and dilation (these last two can sound scary, but they aren’t. See <a href="http://docs.opencv.org/2.4/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.html" class="bare">http://docs.opencv.org/2.4/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.html</a>).</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Gaussian blur corrupts text more than median blur</p>
</li>
<li>
<p>I found that using only dilation yields better average results than any other combination of mentioned techniques.</p>
</li>
<li>
<p>In addition to removing noise, dilation makes text clearer (by making white spaces inside of letter such as ‘a’ or ‘e’ bigger)</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_cropping_second_article">Cropping (second article)</h5>
<div class="ulist">
<ul>
<li>
<p>To achieve quicker processing, the image at this step can be resized not to the height of 2048, but of 600 (I wouldn’t recommend anything less than 500). I haven’t observed any negative effects after introducing this change.</p>
</li>
<li>
<p>Two rank filters can be replaced by a single median blur</p>
</li>
<li>
<p>Shape of the dilation kernel doesn’t affect the result</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Now it’s time to feed the resulting image to the Tesseract. It can be done quite simply with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">pytesseract.image_to_string(Image.open('scan_res.jpg'))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Additionally, I want to mention the problem of measuring accuracy of recognition. Having found nothing on Google in the first five minutes of the search, I’ve coded my own metric, which turned out to be not that good. That forced me to google some more, and so I’ve found a paper that discusses and compares different metrics (on their own dataset, which is quite different from mine: Arabic handwriting isn’t exactly English words in print). It gave me the names of the metrics however, which was good enough. The paper can be found here: <a href="http://www.ijcsi.org/papers/IJCSI-11-3-1-18-26.pdf" class="bare">http://www.ijcsi.org/papers/IJCSI-11-3-1-18-26.pdf</a> .</p>
</div>
<div class="paragraph">
<p>I’ve settled on Jaro-Winkler distance, which gives sufficiently accurate results. Also it’s implemented in jellyfish package, so it’s quite easy to use.<br>
I hope this article will prove itself useful.<br>
For reference, my final code can be found here: <a href="https://github.com/AwesomeLemon/document-recognition" class="bare">https://github.com/AwesomeLemon/document-recognition</a></p>
</div>
</div>]]></description><link>https://AwesomeLemon.github.io/2017/01/15/Document-recognition-with-Python-OpenCV-and-Tesseract.html</link><guid isPermaLink="true">https://AwesomeLemon.github.io/2017/01/15/Document-recognition-with-Python-OpenCV-and-Tesseract.html</guid><category><![CDATA[opencv]]></category><category><![CDATA[ python]]></category><category><![CDATA[ tesseract]]></category><category><![CDATA[ ocr]]></category><category><![CDATA[ open source]]></category><dc:creator><![CDATA[Alexander Chebykin]]></dc:creator><pubDate>Sun, 15 Jan 2017 00:00:00 GMT</pubDate></item></channel></rss>